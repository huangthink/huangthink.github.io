<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Elasticsearch架构原理</title>
  <meta name="description" content="架构原理">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Elasticsearch架构原理">
  <meta name="twitter:description" content="架构原理">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Elasticsearch架构原理">
  <meta property="og:description" content="架构原理">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/elasticsearch-architecture/">
  <link rel="alternate" type="application/rss+xml" title="Trendy Man" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 Trendy Man 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Trendy Man logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Trendy Man" class="blog-button">Trendy Man</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">a backend developer</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">万物之中 希望至美 至美之物 永不凋零</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        <p class="panel-cover__description">Life@Shanghai Work@Meituan</p>
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">博客</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/princecharming" title="@princecharming 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/trendystar" title="@trendystar 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/huangthink" title="@huangthink" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:huangthink@gmail.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-gray-lightest"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-08-06 14:58:02 +0800" itemprop="datePublished" class="post-meta__date date">2017-08-06</time> &#8226; <span class="post-meta__tags tags">能工巧匠</span>
    </div>
    <h1 class="post-title">Elasticsearch架构原理</h1>
  </header>

  <section class="post">
    <h1 id="架构原理">架构原理</h1>

<p>本书作为 Elastic Stack 指南，关注于 Elasticsearch 在日志和数据分析场景的应用，并不打算对底层的 Lucene 原理或者 Java 编程做详细的介绍，但是 Elasticsearch 层面上的一些架构设计，对我们做性能调优，故障处理，具有非常重要的影响。</p>

<p>所以，作为 ES 部分的起始章节，先从数据流向和分布的层面，介绍一下 ES 的工作原理，以及相关的可控项。各位读者可以跳过这节先行阅读后面的运维操作部分，但作为性能调优的基础知识，依然建议大家抽时间返回来了解。</p>

<h1 id="带着问题学习">带着问题学习</h1>

<ol>
  <li>写入的数据是如何变成elasticsearch里可以被检索和聚合的索引内容的？</li>
  <li>lucene如何实现准实时索引？</li>
  <li>什么是segment？</li>
  <li>什么是commit？</li>
  <li>segment的数据来自哪里？</li>
  <li>segment在写入磁盘前就可以被检索，是因为利用了什么？</li>
  <li>elasticsearch中的refresh操作是什么？配置项是哪个？设置的命令是什么？</li>
  <li>refresh只是写到了文件系统缓存，那么实际写入磁盘是由什么控制呢？，如果这期间发生错误和故障，数据会不会丢失？</li>
  <li>什么是translog日志？什么时候会被清空？什么是flush操作？配置项是什么？怎么配置？
10.什么是段合并？为什么要段合并？段合并线程配置项？段合并策略？怎么forcemerge(optimize)？</li>
  <li>routing的规则是什么样的？replica读写过程？wait_for_active_shards参数timeout参数 ？</li>
  <li>reroute 接口？</li>
  <li>两种 自动发现方式？</li>
</ol>

<h1 id="segmentbuffer和translog对实时性的影响">segment、buffer和translog对实时性的影响</h1>

<p>既然介绍数据流向，首先第一步就是：写入的数据是如何变成 Elasticsearch 里可以被检索和聚合的索引内容的？</p>

<p>以单文件的静态层面看，每个全文索引都是一个词元的倒排索引，具体涉及到全文索引的通用知识，这里不单独介绍，有兴趣的读者可以阅读《Lucene in Action》等书籍详细了解。</p>

<h2 id="动态更新的-lucene-索引">动态更新的 Lucene 索引</h2>

<p>以在线动态服务的层面看，要做到实时更新条件下数据的可用和可靠，就需要在倒排索引的基础上，再做一系列更高级的处理。</p>

<p>其实总结一下 Lucene 的处理办法，很简单，就是一句话：<strong>新收到的数据写到新的索引文件里</strong>。</p>

<p>Lucene 把每次生成的倒排索引，叫做一个段(segment)。然后另外使用一个 commit 文件，记录索引内所有的 segment。而生成 segment 的数据来源，则是内存中的 buffer。也就是说，动态更新过程如下：</p>

<ol>
  <li>
    <p>当前索引有 3 个 segment 可用。索引状态如图 2-1；
<img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1101.png" alt="A Lucene index with a commit point and three segments" />
图 2-1</p>
  </li>
  <li>
    <p>新接收的数据进入内存 buffer。索引状态如图 2-2；
<img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1102.png" alt="A Lucene index with new documents in the in-memory buffer, ready to commit" />
图 2-2</p>
  </li>
  <li>
    <p>内存 buffer 刷到磁盘，生成一个新的 segment，commit 文件同步更新。索引状态如图 2-3。
<img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1103.png" alt="After a commit, a new segment is added to the index and the buffer is cleared" />
图 2-3</p>
  </li>
</ol>

<h2 id="利用磁盘缓存实现的准实时检索">利用磁盘缓存实现的准实时检索</h2>

<p>既然涉及到磁盘，那么一个不可避免的问题就来了：磁盘太慢了！对我们要求实时性很高的服务来说，这种处理还不够。所以，在第 3 步的处理中，还有一个中间状态：</p>

<ol>
  <li>内存 buffer 生成一个新的 segment，刷到文件系统缓存中，Lucene 即可检索这个新 segment。索引状态如图 2-4。
<img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1105.png" alt="The buffer contents have been written to a segment, which is searchable, but is not yet commited" />
图 2-4</li>
  <li>文件系统缓存真正同步到磁盘上，commit 文件更新。达到图 2-3 中的状态。</li>
</ol>

<p>这一步刷到文件系统缓存的步骤，在 Elasticsearch 中，是默认设置为 1 秒间隔的，对于大多数应用来说，几乎就相当于是实时可搜索了。Elasticsearch 也提供了单独的 <code class="highlighter-rouge">/_refresh</code> 接口，用户如果对 1 秒间隔还不满意的，可以主动调用该接口来保证搜索可见。</p>

<p><em>注：5.0 中还提供了一个新的请求参数：<code class="highlighter-rouge">?refresh=wait_for</code>，可以在写入数据后不强制刷新但一直等到刷新才返回。</em></p>

<p>不过对于 Elastic Stack 的日志场景来说，恰恰相反，我们并不需要如此高的实时性，而是需要更快的写入性能。所以，一般来说，我们反而会通过 <code class="highlighter-rouge">/_settings</code> 接口或者定制 template 的方式，加大 <code class="highlighter-rouge">refresh_interval</code> 参数：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPOST http://127.0.0.1:9200/logstash-2015.06.21/_settings -d'
{ "refresh_interval": "10s" }
'
</code></pre></div></div>

<p>如果是导入历史数据的场合，那甚至可以先完全关闭掉：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPUT http://127.0.0.1:9200/logstash-2015.05.01 -d'
{
  "settings" : {
    "refresh_interval": "-1"
  }
}'
</code></pre></div></div>

<p>在导入完成以后，修改回来或者手动调用一次即可：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPOST http://127.0.0.1:9200/logstash-2015.05.01/_refresh
</code></pre></div></div>

<h2 id="translog-提供的磁盘同步控制">translog 提供的磁盘同步控制</h2>

<p>既然 refresh 只是写到文件系统缓存，那么第 4 步写到实际磁盘又是有什么来控制的？如果这期间发生主机错误、硬件故障等异常情况，数据会不会丢失？</p>

<p>这里，其实有另一个机制来控制。Elasticsearch 在把数据写入到内存 buffer 的同时，其实还另外记录了一个 translog 日志。也就是说，第 2 步并不是图 2-2 的状态，而是像图 2-5 这样：</p>

<p><img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1106.png" alt="New documents are added to the in-memory buffer and appended to the transaction log" />
图 2-5</p>

<p>在第 3 和第 4 步，refresh 发生的时候，translog 日志文件依然保持原样，如图 2-6：</p>

<p><img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1108.png" alt="The transaction log keeps accumulating documents" />
图 2-6</p>

<p>也就是说，如果在这期间发生异常，Elasticsearch 会从 commit 位置开始，恢复整个 translog 文件中的记录，保证数据一致性。</p>

<p>等到真正把 segment 刷到磁盘，且 commit 文件进行更新的时候， translog 文件才清空。这一步，叫做 flush。同样，Elasticsearch 也提供了 <code class="highlighter-rouge">/_flush</code> 接口。</p>

<p>对于 flush 操作，Elasticsearch 默认设置为：每 30 分钟主动进行一次 flush，或者当 translog 文件大小大于 512MB (老版本是 200MB)时，主动进行一次 flush。这两个行为，可以分别通过 <code class="highlighter-rouge">index.translog.flush_threshold_period</code> 和 <code class="highlighter-rouge">index.translog.flush_threshold_size</code> 参数修改。</p>

<p>如果对这两种控制方式都不满意，Elasticsearch 还可以通过 <code class="highlighter-rouge">index.translog.flush_threshold_ops</code> 参数，控制每收到多少条数据后 flush 一次。</p>

<h3 id="translog-的一致性">translog 的一致性</h3>

<p>索引数据的一致性通过 translog 保证。那么 translog 文件自己呢？</p>

<p>默认情况下，Elasticsearch 每 5 秒，或每次请求操作结束前，会强制刷新 translog 日志到磁盘上。</p>

<p>后者是 Elasticsearch 2.0 新加入的特性。为了保证不丢数据，每次 index、bulk、delete、update 完成的时候，一定触发刷新 translog 到磁盘上，才给请求返回 200 OK。这个改变在提高数据安全性的同时当然也降低了一点性能。</p>

<p>如果你不在意这点可能性，还是希望性能优先，可以在 index template 里设置如下参数：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "index.translog.durability": "async"
}
</code></pre></div></div>

<h2 id="elasticsearch-分布式索引">Elasticsearch 分布式索引</h2>

<p>大家可能注意到了，前面一段内容，一直写的是”Lucene 索引”。这个区别在于，Elasticsearch 为了完成分布式系统，对一些名词概念作了变动。<em>索引_成为了整个集群级别的命名，而在单个主机上的_Lucene 索引</em>，则被命名为_分片(shard)_。至于数据是怎么识别到自己应该在哪个分片，请阅读稍后有关 routing 的章节。</p>

<h1 id="segment-merge对写入性能的影响">segment merge对写入性能的影响</h1>

<p>通过上节内容，我们知道了数据怎么进入 ES 并且如何才能让数据更快的被检索使用。其中用一句话概括了 Lucene 的设计思路就是”开新文件”。从另一个方面看，开新文件也会给服务器带来负载压力。因为默认每 1 秒，都会有一个新文件产生，每个文件都需要有文件句柄，内存，CPU 使用等各种资源。一天有 86400 秒，设想一下，每次请求要扫描一遍 86400 个文件，这个响应性能绝对好不了！</p>

<p>为了解决这个问题，ES 会不断在后台运行任务，主动将这些零散的 segment 做数据归并，尽量让索引内只保有少量的，每个都比较大的，segment 文件。这个过程是有独立的线程来进行的，并不影响新 segment 的产生。归并过程中，索引状态如图 2-7，尚未完成的较大的 segment 是被排除在检索可见范围之外的：</p>

<p><img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1110.png" alt="Two commited segments and one uncommited segment in the process of being merged into a bigger segment" />
图 2-7</p>

<p>当归并完成，较大的这个 segment 刷到磁盘后，commit 文件做出相应变更，删除之前几个小 segment，改成新的大 segment。等检索请求都从小 segment 转到大 segment 上以后，删除没用的小 segment。这时候，索引里 segment 数量就下降了，状态如图 2-8 所示：</p>

<p><img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_1111.png" alt="Once merging has finished, the old segments are deleted" />
图 2-8</p>

<h2 id="归并线程配置">归并线程配置</h2>

<p>segment 归并的过程，需要先读取 segment，归并计算，再写一遍 segment，最后还要保证刷到磁盘。可以说，这是一个非常消耗磁盘 IO 和 CPU 的任务。所以，ES 提供了对归并线程的限速机制，确保这个任务不会过分影响到其他任务。</p>

<p>在 5.0 之前，归并线程的限速配置 <code class="highlighter-rouge">indices.store.throttle.max_bytes_per_sec</code> 是 20MB。对于写入量较大，磁盘转速较高，甚至使用 SSD 盘的服务器来说，这个限速是明显过低的。对于 Elastic Stack 应用，社区广泛的建议是可以适当调大到 100MB或者更高。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPUT http://127.0.0.1:9200/_cluster/settings -d'
{
    "persistent" : {
        "indices.store.throttle.max_bytes_per_sec" : "100mb"
    }
}'
</code></pre></div></div>

<p>5.0 开始，ES 对此作了大幅度改进，使用了 Lucene 的 CMS(ConcurrentMergeScheduler) 的 auto throttle 机制，正常情况下已经不再需要手动配置 <code class="highlighter-rouge">indices.store.throttle.max_bytes_per_sec</code> 了。官方文档中都已经删除了相关介绍，不过从源码中还是可以看到，这个值目前的默认设置是 10240 MB。</p>

<p>归并线程的数目，ES 也是有所控制的。默认数目的计算公式是： <code class="highlighter-rouge">Math.min(3, Runtime.getRuntime().availableProcessors() / 2)</code>。即服务器 CPU 核数的一半大于 3 时，启动 3 个归并线程；否则启动跟 CPU 核数的一半相等的线程数。相信一般做 Elastic Stack 的服务器 CPU 合数都会在 6 个以上。所以一般来说就是 3 个归并线程。如果你确定自己磁盘性能跟不上，可以降低 <code class="highlighter-rouge">index.merge.scheduler.max_thread_count</code> 配置，免得 IO 情况更加恶化。</p>

<h2 id="归并策略">归并策略</h2>

<p>归并线程是按照一定的运行策略来挑选 segment 进行归并的。主要有以下几条：</p>

<ul>
  <li>index.merge.policy.floor_segment
默认 2MB，小于这个大小的 segment，优先被归并。</li>
  <li>index.merge.policy.max_merge_at_once
默认一次最多归并 10 个 segment</li>
  <li>index.merge.policy.max_merge_at_once_explicit
默认 forcemerge 时一次最多归并 30 个 segment。</li>
  <li>index.merge.policy.max_merged_segment
默认 5 GB，大于这个大小的 segment，不用参与归并。forcemerge 除外。</li>
</ul>

<p>根据这段策略，其实我们也可以从另一个角度考虑如何减少 segment 归并的消耗以及提高响应的办法：加大 flush 间隔，尽量让每次新生成的 segment 本身大小就比较大。</p>

<h2 id="forcemerge-接口">forcemerge 接口</h2>

<p>既然默认的最大 segment 大小是 5GB。那么一个比较庞大的数据索引，就必然会有为数不少的 segment 永远存在，这对文件句柄，内存等资源都是极大的浪费。但是由于归并任务太消耗资源，所以一般不太选择加大 <code class="highlighter-rouge">index.merge.policy.max_merged_segment</code> 配置，而是在负载较低的时间段，通过 forcemerge 接口，强制归并 segment。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPOST http://127.0.0.1:9200/logstash-2015-06.10/_forcemerge?max_num_segments=1
</code></pre></div></div>

<p>由于 forcemerge 线程对资源的消耗比普通的归并线程大得多，所以，绝对不建议对还在写入数据的热索引执行这个操作。这个问题对于 Elastic Stack 来说非常好办，一般索引都是按天分割的。更合适的任务定义方式，请阅读本书稍后的 curator 章节。</p>

<h1 id="routing和replica的读写过程">routing和replica的读写过程</h1>

<p>之前两节，完整介绍了在单个 Lucene 索引，即 ES 分片内的数据写入流程。现在彻底回到 ES 的分布式层面上来，当一个 ES 节点收到一条数据的写入请求时，它是如何确认这个数据应该存储在哪个节点的哪个分片上的？</p>

<h2 id="路由计算">路由计算</h2>

<p>作为一个没有额外依赖的简单的分布式方案，ES 在这个问题上同样选择了一个非常简洁的处理方式，对任一条数据计算其对应分片的方式如下：</p>

<blockquote>
  <p>shard = hash(routing) % number_of_primary_shards</p>
</blockquote>

<p>每个数据都有一个 routing 参数，默认情况下，就使用其 <code class="highlighter-rouge">_id</code> 值。将其 <code class="highlighter-rouge">_id</code> 值计算哈希后，对索引的主分片数取余，就是数据实际应该存储到的分片 ID。</p>

<p>由于取余这个计算，完全依赖于分母，所以导致 ES 索引有一个限制，索引的主分片数，不可以随意修改。因为一旦主分片数不一样，所以数据的存储位置计算结果都会发生改变，索引数据就完全不可读了。</p>

<h2 id="副本一致性">副本一致性</h2>

<p>作为分布式系统，数据副本可算是一个标配。ES 数据写入流程，自然也涉及到副本。在有副本配置的情况下，数据从发向 ES 节点，到接到 ES 节点响应返回，流向如下(附图 2-9)：</p>

<ol>
  <li>客户端请求发送给 Node 1 节点，注意图中 Node 1 是 Master 节点，实际完全可以不是。</li>
  <li>Node 1 用数据的 <code class="highlighter-rouge">_id</code> 取余计算得到应该讲数据存储到 shard 0 上。通过 cluster state 信息发现 shard 0 的主分片已经分配到了 Node 3 上。Node 1 转发请求数据给 Node 3。</li>
  <li>Node 3 完成请求数据的索引过程，存入主分片 0。然后并行转发数据给分配有 shard 0 的副本分片的 Node 1 和 Node 2。当收到任一节点汇报副本分片数据写入成功，Node 3 即返回给初始的接收节点 Node 1，宣布数据写入成功。Node 1 返回成功响应给客户端。</li>
</ol>

<p><img src="https://www.elastic.co/guide/en/elasticsearch/guide/current/images/elas_0402.png" alt="Creating, indexing or deleting a single document" />
图 2-9</p>

<p>这个过程中，有几个参数可以用来控制或变更其行为：</p>

<ul>
  <li>wait_for_active_shards
上面示例中，2 个副本分片只要有 1 个成功，就可以返回给客户端了。这点也是有配置项的。其默认值的计算来源如下：
    <blockquote>
      <p>int( (primary + number_of_replicas) / 2 ) + 1</p>
    </blockquote>
  </li>
</ul>

<p>根据需要，也可以将参数设置为 one，表示仅写完主分片就返回，等同于 async；还可以设置为 all，表示等所有副本分片都写完才能返回。</p>

<ul>
  <li>timeout
如果集群出现异常，有些分片当前不可用，ES 默认会等待 1 分钟看分片能否恢复。可以使用 <code class="highlighter-rouge">?timeout=30s</code> 参数来缩短这个等待时间。</li>
</ul>

<p>副本配置和分片配置不一样，是可以随时调整的。有些较大的索引，甚至可以在做 forcemerge 前，先把副本全部取消掉，等 optimize 完后，再重新开启副本，节约单个 segment 的重复归并消耗。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPUT http://127.0.0.1:9200/logstash-mweibo-2015.05.02/_settings -d '{
    "index": { "number_of_replicas" : 0 }
}'
</code></pre></div></div>

<h1 id="shard-的-allocate-控制">shard 的 allocate 控制</h1>

<p>某个 shard 分配在哪个节点上，一般来说，是由 ES 自动决定的。以下几种情况会触发分配动作：</p>

<ol>
  <li>新索引生成</li>
  <li>索引的删除</li>
  <li>新增副本分片</li>
  <li>节点增减引发的数据均衡</li>
</ol>

<p>ES 提供了一系列参数详细控制这部分逻辑：</p>

<ul>
  <li>cluster.routing.allocation.enable
该参数用来控制允许分配哪种分片。默认是 <code class="highlighter-rouge">all</code>。可选项还包括 <code class="highlighter-rouge">primaries</code> 和 <code class="highlighter-rouge">new_primaries</code>。<code class="highlighter-rouge">none</code> 则彻底拒绝分片。该参数的作用，本书稍后集群升级章节会有说明。</li>
  <li>cluster.routing.allocation.allow_rebalance
该参数用来控制什么时候允许数据均衡。默认是 <code class="highlighter-rouge">indices_all_active</code>，即要求所有分片都正常启动成功以后，才可以进行数据均衡操作，否则的话，在集群重启阶段，会浪费太多流量了。</li>
  <li>cluster.routing.allocation.cluster_concurrent_rebalance
该参数用来控制<strong>集群内</strong>同时运行的数据均衡任务个数。默认是 2 个。如果有节点增减，且集群负载压力不高的时候，可以适当加大。</li>
  <li>cluster.routing.allocation.node_initial_primaries_recoveries
该参数用来控制<strong>节点</strong>重启时，允许同时恢复几个主分片。默认是 4 个。如果节点是多磁盘，且 IO 压力不大，可以适当加大。</li>
  <li>cluster.routing.allocation.node_concurrent_recoveries
该参数用来控制<strong>节点</strong>除了主分片重启恢复以外其他情况下，允许同时运行的数据恢复任务。默认是 2 个。所以，节点重启时，可以看到主分片迅速恢复完成，副本分片的恢复却很慢。除了副本分片本身数据要通过网络复制以外，并发线程本身也减少了一半。当然，这种设置也是有道理的——主分片一定是本地恢复，副本分片却需要走网络，带宽是有限的。从 ES 1.6 开始，冷索引的副本分片可以本地恢复，这个参数也就是可以适当加大了。</li>
  <li>indices.recovery.concurrent_streams
该参数用来控制<strong>节点</strong>从网络复制恢复副本分片时的数据流个数。默认是 3 个。可以配合上一条配置一起加大。</li>
  <li>indices.recovery.max_bytes_per_sec
该参数用来控制<strong>节点</strong>恢复时的速率。默认是 40MB。显然是比较小的，建议加大。</li>
</ul>

<p>此外，ES 还有一些其他的分片分配控制策略。比如以 <code class="highlighter-rouge">tag</code> 和 <code class="highlighter-rouge">rack_id</code> 作为区分等。一般来说，Elastic Stack 场景中使用不多。运维人员可能比较常见的策略有两种：</p>

<ol>
  <li>磁盘限额
为了保护节点数据安全，ES 会定时(<code class="highlighter-rouge">cluster.info.update.interval</code>，默认 30 秒)检查一下各节点的数据目录磁盘使用情况。在达到 <code class="highlighter-rouge">cluster.routing.allocation.disk.watermark.low</code> (默认 85%)的时候，新索引分片就不会再分配到这个节点上了。在达到 <code class="highlighter-rouge">cluster.routing.allocation.disk.watermark.high</code> (默认 90%)的时候，就会触发该节点现存分片的数据均衡，把数据挪到其他节点上去。这两个值不但可以写百分比，还可以写具体的字节数。有些公司可能出于成本考虑，对磁盘使用率有一定的要求，需要适当抬高这个配置：</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.watermark.low" : "85%",
        "cluster.routing.allocation.disk.watermark.high" : "10gb",
        "cluster.info.update.interval" : "1m"
    }
}'
</code></pre></div></div>

<ol>
  <li>热索引分片不均
默认情况下，ES 集群的数据均衡策略是以各节点的分片总数(<em>indices_all_active</em>)作为基准的。这对于搜索服务来说无疑是均衡搜索压力提高性能的好办法。但是对于 Elastic Stack 场景，一般压力集中在新索引的数据写入方面。正常运行的时候，也没有问题。但是当集群扩容时，新加入集群的节点，分片总数远远低于其他节点。这时候如果有新索引创建，ES 的默认策略会导致新索引的所有主分片几乎全分配在这台新节点上。整个集群的写入压力，压在一个节点上，结果很可能是这个节点直接被压死，集群出现异常。
所以，对于 Elastic Stack 场景，强烈建议大家预先计算好索引的分片数后，配置好单节点分片的限额。比如，一个 5 节点的集群，索引主分片 10 个，副本 1 份。则平均下来每个节点应该有 4 个分片，那么就配置：</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -s -XPUT http://127.0.0.1:9200/logstash-2015.05.08/_settings -d '{
    "index": { "routing.allocation.total_shards_per_node" : "5" }
}'
</code></pre></div></div>

<p>注意，这里配置的是 5 而不是 4。因为我们需要预防有机器故障，分片发生迁移的情况。如果写的是 4，那么分片迁移会失败。</p>

<p>此外，另一种方式则更加玄妙，Elasticsearch 中有一系列参数，相互影响，最终联合决定分片分配：</p>

<ul>
  <li>cluster.routing.allocation.balance.shard
节点上分配分片的权重，默认为 0.45。数值越大越倾向于在节点层面均衡分片。</li>
  <li>cluster.routing.allocation.balance.index
每个索引往单个节点上分配分片的权重，默认为 0.55。数值越大越倾向于在索引层面均衡分片。</li>
  <li>cluster.routing.allocation.balance.threshold
大于阈值则触发均衡操作。默认为1。</li>
</ul>

<p>Elasticsearch 中的计算方法是：</p>

<p>(indexBalance _ (node.numShards(index) – avgShardsPerNode(index)) + shardBalance _ (node.numShards() – avgShardsPerNode)) &lt;=&gt; weightthreshold</p>

<p>所以，也可以采取加大 <code class="highlighter-rouge">cluster.routing.allocation.balance.index</code>，甚至设置 <code class="highlighter-rouge">cluster.routing.allocation.balance.shard</code> 为 0 来尽量采用索引内的节点均衡。</p>

<h2 id="reroute-接口">reroute 接口</h2>

<p>上面说的各种配置，都是从策略层面，控制分片分配的选择。在必要的时候，还可以通过 ES 的 reroute 接口，手动完成对分片的分配选择的控制。</p>

<p>reroute 接口支持五种指令：<code class="highlighter-rouge">allocate_replica</code>, <code class="highlighter-rouge">allocate_stale_primary</code>, <code class="highlighter-rouge">allocate_empty_primary</code>，<code class="highlighter-rouge">move</code> 和 <code class="highlighter-rouge">cancel</code>。常用的一般是 allocate 和 move：</p>

<ul>
  <li><code class="highlighter-rouge">allocate_*</code> 指令</li>
</ul>

<p>因为负载过高等原因，有时候个别分片可能长期处于 UNASSIGNED 状态，我们就可以手动分配分片到指定节点上。默认情况下只允许手动分配副本分片(即使用 <code class="highlighter-rouge">allocate_replica</code>)，所以如果要分配主分片，需要单独加一个 <code class="highlighter-rouge">accept_data_loss</code> 选项：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># curl -XPOST 127.0.0.1:9200/_cluster/reroute -d '{
  "commands" : [ {
        "allocate_stale_primary" :
            {
              "index" : "logstash-2015.05.27", "shard" : 61, "node" : "10.19.0.77", "accept_data_loss" : true
            }
        }
  ]
}'
</code></pre></div></div>

<p>注意，<code class="highlighter-rouge">allocate_stale_primary</code> 表示准备分配到的节点上可能有老版本的历史数据，运行时请提前确认一下是哪个节点上保留有这个分片的实际目录，且目录大小最大。然后手动分配到这个节点上。以此减少数据丢失。</p>

<ul>
  <li>move 指令</li>
</ul>

<p>因为负载过高，磁盘利用率过高，服务器下线，更换磁盘等原因，可以会需要从节点上移走部分分片：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -XPOST 127.0.0.1:9200/_cluster/reroute -d '{
  "commands" : [ {
        "move" :
            {
              "index" : "logstash-2015.05.22", "shard" : 0, "from_node" : "10.19.0.81", "to_node" : "10.19.0.104"
            }
        }
  ]
}'
</code></pre></div></div>

<h2 id="分配失败原因">分配失败原因</h2>

<p>如果是自己手工 reroute 失败，Elasticsearch 返回的响应中会带上失败的原因。不过格式非常难看，一堆 YES，NO。从 5.0 版本开始，Elasticsearch 新增了一个 allocation explain 接口，专门用来解释指定分片的具体失败理由：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -XGET 'http://localhost:9200/_cluster/allocation/explain' -d'{
      "index": "logstash-2016.10.31",
      "shard": 0,
      "primary": false
}'
</code></pre></div></div>

<p>得到的响应如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "shard" : {
        "index" : "myindex",
        "index_uuid" : "KnW0-zELRs6PK84l0r38ZA",
        "id" : 0,
        "primary" : false
    },
    "assigned" : false,
    "shard_state_fetch_pending": false,
    "unassigned_info" : {
        "reason" : "INDEX_CREATED",
        "at" : "2016-03-22T20:04:23.620Z"
    },
    "allocation_delay_ms" : 0,
    "remaining_delay_ms" : 0,
    "nodes" : {
        "V-Spi0AyRZ6ZvKbaI3691w" : {
            "node_name" : "H5dfFeA",
            "node_attributes" : {
                "bar" : "baz"
            },
            "store" : {
                "shard_copy" : "NONE"
            },
            "final_decision" : "NO",
            "final_explanation" : "the shard cannot be assigned because one or more allocation decider returns a 'NO' decision",
            "weight" : 0.06666675,
            "decisions" : [ {
                "decider" : "filter",
                "decision" : "NO",
                "explanation" : "node does not match index include filters [foo:\"bar\"]"
            }  ]
        },
        "Qc6VL8c5RWaw1qXZ0Rg57g" : {
            ...
</code></pre></div></div>
<p>这会是很长一串 JSON，把集群里所有的节点都列上来，挨个解释为什么不能分配到这个节点。</p>

<h2 id="节点下线">节点下线</h2>

<p>集群中个别节点出现故障预警等情况，需要下线，也是 Elasticsearch 运维工作中常见的情况。如果已经稳定运行过一段时间的集群，每个节点上都会保存有数量不少的分片。这种时候通过 reroute 接口手动转移，就显得太过麻烦了。这个时候，有另一种方式：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -XPUT 127.0.0.1:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
   }
}'
</code></pre></div></div>

<p>Elasticsearch 集群就会自动把这个 IP 上的所有分片，都自动转移到其他节点上。等到转移完成，这个空节点就可以毫无影响的下线了。</p>

<p>和 <code class="highlighter-rouge">_ip</code> 类似的参数还有 <code class="highlighter-rouge">_host</code>, <code class="highlighter-rouge">_name</code> 等。此外，这类参数不单是 cluster 级别，也可以是 index 级别。下一小节就是 index 级别的用例。</p>

<h2 id="冷热数据的读写分离">冷热数据的读写分离</h2>

<p>Elasticsearch 集群一个比较突出的问题是: 用户做一次大的查询的时候, 非常大量的读 IO 以及聚合计算导致机器 Load 升高, CPU 使用率上升, 会影响阻塞到新数据的写入, 这个过程甚至会持续几分钟。所以，可能需要仿照 MySQL 集群一样，做读写分离。</p>

<h3 id="实施方案">实施方案</h3>

<ol>
  <li>N 台机器做热数据的存储, 上面只放当天的数据。这 N 台热数据节点上面的 elasticsearc.yml 中配置 <code class="highlighter-rouge">node.attr.tag: hot</code></li>
  <li>之前的数据放在另外的 M 台机器上。这 M 台冷数据节点中配置 <code class="highlighter-rouge">node.attr.tag: stale</code></li>
  <li>模板中控制对新建索引添加 hot 标签：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
"order" : 0,
"template" : "*",
"settings" : {
  "index.routing.allocation.include.tag" : "hot"
}
}
</code></pre></div>    </div>
  </li>
  <li>每天计划任务更新索引的配置, 将 tag 更改为 stale, 索引会自动迁移到 M 台冷数据节点</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -XPUT http://127.0.0.1:9200/indexname/_settings -d'
{
   "index": {
      "routing": {
         "allocation": {
            "include": {
               "tag": "stale"
            }
         }
     }
   }
}'
</code></pre></div></div>

<p>这样，写操作集中在 N 台热数据节点上，大范围的读操作集中在 M 台冷数据节点上。避免了堵塞影响。</p>

<p>该方案运用的，是 Elasticsearch 中的 allocation filter 功能，详细说明见：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/shard-allocation-filtering.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/shard-allocation-filtering.html</a></p>

<h1 id="集群自动发现">集群自动发现</h1>

<p>ES 是一个 P2P 类型(使用 gossip 协议)的分布式系统，除了集群状态管理以外，其他所有的请求都可以发送到集群内任意一台节点上，这个节点可以自己找到需要转发给哪些节点，并且直接跟这些节点通信。</p>

<p>所以，从网络架构及服务配置上来说，构建集群所需要的配置极其简单。在 Elasticsearch 2.0 之前，无阻碍的网络下，所有配置了相同 <code class="highlighter-rouge">cluster.name</code> 的节点都自动归属到一个集群中。</p>

<p>2.0 版本之后，基于安全的考虑，Elasticsearch 稍作了调整，避免开发环境过于随便造成的麻烦。</p>

<h2 id="unicast-方式">unicast 方式</h2>

<p>ES 从 2.0 版本开始，默认的自动发现方式改为了单播(unicast)方式。配置里提供几台节点的地址，ES 将其视作 gossip router 角色，借以完成集群的发现。由于这只是 ES 内一个很小的功能，所以 gossip router 角色并不需要单独配置，每个 ES 节点都可以担任。所以，采用单播方式的集群，各节点都配置相同的几个节点列表作为 router 即可。</p>

<p>此外，考虑到节点有时候因为高负载，慢 GC 等原因可能会有偶尔没及时响应 ping 包的可能，一般建议稍微加大 Fault Detection 的超时时间。</p>

<p>同样基于安全考虑做的变更还有监听的主机名。现在默认只监听本地 lo 网卡上。所以正式环境上需要修改配置为监听具体的网卡。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>network.host: "192.168.0.2" 
discovery.zen.minimum_master_nodes: 3
discovery.zen.ping_timeout: 100s
discovery.zen.fd.ping_timeout: 100s
discovery.zen.ping.unicast.hosts: ["10.19.0.97","10.19.0.98","10.19.0.99","10.19.0.100"]
</code></pre></div></div>

<p>上面的配置中，两个 timeout 可能会让人有所迷惑。这里的 <strong>fd</strong> 是 fault detection 的缩写。也就是说：</p>

<ul>
  <li>discovery.zen.ping_timeout 参数仅在加入或者选举 master 主节点的时候才起作用；</li>
  <li>discovery.zen.fd.ping_timeout 参数则在稳定运行的集群中，master 检测所有节点，以及节点检测 master 是否畅通时长期有用。</li>
</ul>

<p>既然是长期有用，自然还有运行间隔和重试的配置，也可以根据实际情况调整：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>discovery.zen.fd.ping_interval: 10s
discovery.zen.fd.ping_retries: 10
</code></pre></div></div>


  </section>
</article>

<!--<section class="read-more">
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/elastic-stack/" title="link to Elastic Stack安装教程">Elastic Stack安装教程</a></h2>
       <p class="excerpt">环境准备  CentOS7.1  JDK1.8  elasticsearch-5.4.3JDK安装配置下载地址：http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gzelasticsearch安装配置下载解压使用命令从官网下载最新版本的Elasticsearch压缩包cd /var/wdwget -N https://artifacts.elastic.co/downloads/ela...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-05-18 11:30:28 +0800" class="post-list__meta--date date">2017-05-18</time> &#8226; <span class="post-list__meta--tags tags">能工巧匠</span><a class="btn-border-small" href=/elastic-stack/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://localhost:4000/elasticsearch-architecture/";
        this.page.identifier = "/elasticsearch-architecture/";
    };

    var disqus_shortname = 'felics';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>
-->

            <section class="footer">
    <footer>
    	<span class="footer__copyright">&copy; 2021  Trendy Man.  本博客由<a href="https://jekyllrb.com"> Jekyll </a>生成 使用<a href="https://github.com/onevcat/vno-jekyll"> Vno-Jekyll </a>作为主题</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
